# Machine Learning Basics
Basics for Machine Learning

# About
This repository contains python implementations of some of the fundamental Machine Learning models and algorithms based off of Erik Linder-Noren's post. 

Note, the purpose of this is not to produce as optimized and computationally efficient algorithms as possible,
but rather to present the inner workings of them in a clear and accessible way.

# Confidence Intervals 

Added in a Study Guide about basic statistcs using point estimators, confidence intervals, and such. (More Examples)[http://stat.math.uregina.ca/~kozdron/Teaching/Regina/252Winter16/Handouts/ch3.pdf] 

# Support Vectors

Added in are the study guide and intuition behind Support Vector. There are a lot of websites I used to help fully understand SVM as well as its implementation. 

Also added in example scripts using the sklearn libraries

# SVM Links

[Introduction to Support Vector Machines](https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72)
(https://www.ritchieng.com/machine-learning-svms-support-vector-machines/)

[SVM Notes](https://www.ritchieng.com/machine-learning-svms-support-vector-machines/)

[Ease into SVM](https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/)

[How to derive the cost function from Logisitc Regression](https://stats.stackexchange.com/questions/278771/how-is-the-cost-function-from-logistic-regression-derivated)

[Regularization in Logisitc Regression](https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html)

[Kernel method](https://en.wikipedia.org/wiki/Kernel_method)

[Similarity measure](https://en.wikipedia.org/wiki/Similarity_measure)

[The Kernel Trick](https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf)

[Support Vector Machines](https://en.wikipedia.org/wiki/Support-vector_machine)

[Math Behind SVM](http://mlwiki.org/index.php/Support_Vector_Machines#Math_Behind_It)

[Formal Introduction to SVM by Andrew Ng](http://cs229.stanford.edu/notes/cs229-notes3.pdf)

[The Support Vector Machine and regularization MIT Notes](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/lec4.pdf)

[Kernel and the Kernal Trick](https://svivek.com/teaching/machine-learning/fall2018/slides/svm/kernels.pdf)

[Gram Matrix](http://mlwiki.org/index.php/Gram_Matrices)

[Questions and Answers: Imperative for SVM](https://medium.com/datadriveninvestor/support-vector-machines-important-questions-a47224692495)

# Cross Validation Resources
[How do you know which method of Cross Validation is best?](https://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best)

[Cross-validation basics](https://en.wikipedia.org/wiki/Cross-validation_(statistics))

[What's the difference between K-Fold, Random Subsampling and "Leave one out" approach for Cross Validation?](https://www.quora.com/Whats-the-difference-between-K-Fold-Random-Subsampling-and-Leave-one-out-approach-for-Cross-Validation)

[Cross-Validation in Machine Learning](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f)


# Resouces
Added in lots of resource for learning Machine Learning

[OpenML](https://www.openml.org/home)

[Bias-Variance-Tradeoff](https://elitedatascience.com/bias-variance-tradeoff)

[Dimensionality Reduction Algorithms: Strengths and Weaknesses](https://elitedatascience.com/dimensionality-reduction-algorithms)

[Python for Data Science](https://elitedatascience.com/learn-python-for-data-science)

[Python Quickstart Guide for Data Science](https://elitedatascience.com/python-quickstart)

[9 Mistakes to Avoid When Starting Career in Data Science](https://elitedatascience.com/beginner-mistakes)

[The Hitchhikerâ€™s Guide to Machine Learning in Python](https://medium.freecodecamp.org/the-hitchhikers-guide-to-machine-learning-algorithms-in-python-bfad66adb378)

# References
Thank you Erik Linder-Noren (https://github.com/eriklindernoren/ML-From-Scratch) for you intuition, code, and findings.
